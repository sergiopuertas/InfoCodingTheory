{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sbddNKfu14bM"
   },
   "source": [
    "# Project 1 - Information measures\n",
    "\n",
    "The goal of this first project is to get accustomed to the information and uncertainty measures. We ask you to write a brief report (pdf format) collecting your answers to the different questions. All codes must be written in Python inside this Jupyter Notebook. No other code file will be accepted. Note that you can not change the content of locked cells or import any extra Python library than the ones already imported (numpy and pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IPhOrEZC14bP"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "In this project, you will need to use information measures to answer several questions. Therefore, in this first part, you are asked to write several functions that implement some of the main measures seen in the first theoretical lectures. Remember that you need to fill in this Jupyter Notebook to answer these questions. Pay particular attention to the required output format of each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1ZGhdEwn14bP",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.419312Z",
     "start_time": "2024-03-20T20:15:34.341053Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Locked Cell] You can not import any extra Python library in this Notebook.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JFiTJaFG14bQ"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Write a function entropy that computes the entropy $\\mathcal{H(X)}$ of a random variable $\\mathcal{X}$ from its probability distribution $P_\\mathcal{X} = (p_1, p_2, . . . , p_n)$. Give the mathematical formula that you are using and explain the key parts of your implementation. Intuitively, what is measured by the entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "_4WYMYKx14bR",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.427816Z",
     "start_time": "2024-03-20T20:15:34.346006Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(Px):\n",
    "    \"\"\"\n",
    "    Computes the entropy from the marginal probability distribution. \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Px :  Marginal probability distribution of the random \n",
    "            variable X in a numpy array where Px[i]=P(X=i)\n",
    "    Return:\n",
    "    -------\n",
    "    - The entropy of X (H(X)) as a number (integer, float or double).\n",
    "    \"\"\"\n",
    "    Px_no_0 = Px.copy()\n",
    "    # replace the probability zero with a value for the log not to raise an error\n",
    "    # this is not a big deal since the value of the log will be absorbed by the multiplication with zero\n",
    "    Px_no_0[Px_no_0 == 0] = 1 \n",
    "    return -np.sum(Px * np.log2(Px_no_0)) # We use the Shannon Entropy formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LJ2Q0y5a14bR"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Write a function joint_entropy that computes the joint entropy $\\mathcal{H(X,Y)}$ of two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Compare the entropy and joint_entropy functions (and their corresponding formulas), what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "qqK4N-_q14bS",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.438100Z",
     "start_time": "2024-03-20T20:15:34.350458Z"
    }
   },
   "outputs": [],
   "source": [
    "def joint_entropy(Pxy):\n",
    "    \"\"\"\n",
    "    Computes the joint entropy from the joint probability distribution.  \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxy:  joint probability distribution of X and Y \n",
    "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
    "    Return:\n",
    "    -------\n",
    "    - The joint entropy H(X,Y) as a number (integer, float or double).\n",
    "    \"\"\"\n",
    "    Pxy_no_0 = Pxy.copy()\n",
    "    for i in range(len(Pxy_no_0)):\n",
    "        Pxy_no_0[i][Pxy_no_0[i] == 0] = 1 # same as before replace the zeroes by a value\n",
    "    # Thanks to numpy 2D-arrays and 1D-arrays are handle the same way using the same functions\n",
    "    return -np.sum(Pxy * np.log2(Pxy_no_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gxqVI5qt14bS"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Write a function conditional_entropy that computes the conditional entropy $\\mathcal{H(X|Y)}$ of a discrete random variable $\\mathcal{X}$ given another discrete random variable $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Describe an equivalent way of computing that quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "P-HBnYxh14bS",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.445760Z",
     "start_time": "2024-03-20T20:15:34.355379Z"
    }
   },
   "outputs": [],
   "source": [
    "def conditional_entropy(Pxy):\n",
    "    \"\"\"\n",
    "    Computes the conditional entropy from the joint probability distribution.\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxy:  joint probability distribution of X and Y \n",
    "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
    "    Return:\n",
    "    -------\n",
    "    - The conditional entropy H(X|Y) as a number (integer, float or double)\n",
    "    \"\"\"\n",
    "    Py = np.sum(Pxy, axis=0)\n",
    "    Py[Py == 0] = 1 # replace the zeroes by non-zeroes, it will have no effect\n",
    "    Px_given_y = Pxy / Py # P[x|y] = P[x, y]/P[y]\n",
    "    Px_given_y[Px_given_y == 0] = 1 # replace the zeroes by non-zeroes, it will have no effect\n",
    "    return -np.sum(Pxy * np.log2(Px_given_y)) # the formula is self explanatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t5a3I5RV14bT"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Write a function mutual_information that computes the mutual information $\\mathcal{I(X;Y)}$ between two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$ . Give the mathematical formula that you are using and explain the key parts of your implementation. What can you deduce from the mutual information $\\mathcal{I(X;Y)}$ on the relationship between $\\mathcal{X}$ and $\\mathcal{Y}$? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1-97kFCi14bT",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.504238Z",
     "start_time": "2024-03-20T20:15:34.360344Z"
    }
   },
   "outputs": [],
   "source": [
    "def mutual_information(Pxy):\n",
    "    \"\"\"\n",
    "    Computes the mutual information I(X;Y) from joint probability distribution\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxy:  joint probability distribution of X and Y \n",
    "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
    "    Return:\n",
    "    -------\n",
    "    - The mutual information I(X;Y) as a number (integer, float or double)\n",
    "    \"\"\"\n",
    "    Px = np.sum(Pxy, axis=1)\n",
    "    Py = np.sum(Pxy, axis=0)\n",
    "    return entropy(Px) + entropy(Py) - joint_entropy(Pxy) # apply the formula I(X, Y) = H(X) + H(Y) - H(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HLC7Qkyk14bU"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Let $\\mathcal{X}$, $\\mathcal{Y}$ and $\\mathcal{Z}$ be three discrete random variables. Write the functions cond_joint_entropy and cond_mutual_information that respectively compute $\\mathcal{H(X,Y|Z)}$ and $\\mathcal{I(X;Y|Z)}$ of two discrete random variable $\\mathcal{X}$, $\\mathcal{Y}$ given another discrete random variable $\\mathcal{Z}$ from their joint probability distribution $P_\\mathcal{X,Y,Z}$. Give the mathematical formulas that you are using and explain the key parts of your implementation.\n",
    "Suggestion: Observe the mathematical definitions of these quantities and think how you could derive them from the joint entropy and the mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "IeWejuNa14bU",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.506258Z",
     "start_time": "2024-03-20T20:15:34.364350Z"
    }
   },
   "outputs": [],
   "source": [
    "def cond_joint_entropy(Pxyz):\n",
    "    \"\"\"\n",
    "    Computes the conditional joint entropy of X, Y knowing Z \n",
    "    from the joint probability distribution Pxyz\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxyz: joint probability distribution of X, Y and Z\n",
    "            in a 3-D array where Pxyz[i][j][k]=P(X=i,Y=j,Z=k)\n",
    "    Return:\n",
    "    -------\n",
    "    - The conditional joint entropy H(X,Y|Z) as a number (integer, float or double)\n",
    "    \n",
    "    \"\"\"\n",
    "    Pz = np.sum(Pxyz, axis=(0,1))\n",
    "    Pz[Pz == 0] = 1 # replace the zeroes by non-zeroes, it will have no effect\n",
    "    Pxy_given_z = Pxyz / Pz\n",
    "    Pxy_given_z[Pxy_given_z == 0] = 1 # replace the zeroes by non-zeroes, it will have no effect\n",
    "    return -np.sum(Pxyz*np.log2(Pxy_given_z)) # H(X,Y|Z) = -sum_xyz(P[X,Y,Z]*log_2(P[X,Y|Z]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "xPKeiYmc14bV",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.507935Z",
     "start_time": "2024-03-20T20:15:34.367664Z"
    }
   },
   "outputs": [],
   "source": [
    "def cond_mutual_information(Pxyz):\n",
    "    \"\"\"\n",
    "    Computes the conditional mutual information of X, Y knowing Z \n",
    "    from joint probability distribution Pxyz\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxyz: joint probability distribution of X, Y and Z\n",
    "            in a 3-D array where Pxyz[i][j][k]=P(X=i,Y=j,Z=k)\n",
    "    Return:\n",
    "    -------\n",
    "    - I(X;Y|Z): The conditional mutual information as a number (integer, float or double)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute the conditional entropy of X given Z\n",
    "    H_X_given_Z = conditional_entropy(np.sum(Pxyz, axis=1))\n",
    "    \n",
    "    # Compute the conditional entropy of Y given Z\n",
    "    H_Y_given_Z = conditional_entropy(np.sum(Pxyz, axis=0))\n",
    "    \n",
    "    # Compute the conditional joint entropy of X and Y given Z\n",
    "    H_XY_given_Z = cond_joint_entropy(Pxyz)\n",
    "    \n",
    "    # Compute the conditional mutual information\n",
    "    return H_X_given_Z + H_Y_given_Z - H_XY_given_Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5TfG2aWt14bV",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.509485Z",
     "start_time": "2024-03-20T20:15:34.371096Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Locked Cell] Evaluation of your functions by the examiner. \n",
    "# You don't have access to the evaluation, this will be done by the examiner.\n",
    "# Therefore, this cell will return nothing for the students.\n",
    "import os\n",
    "if os.path.isfile(\"private_evaluation.py\"):\n",
    "    from private_evaluation import unit_tests\n",
    "    unit_tests(entropy, joint_entropy, conditional_entropy, mutual_information, cond_joint_entropy, cond_mutual_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4MSJsvry14bV"
   },
   "source": [
    "### Campaign outcome\n",
    "\n",
    "You may create cells below to answer the different questions related to campaign outcome. Unlike in the first part (Implementation), you are free to define as many cells as you need below to answer the different questions. Try to be structured and clear in your code (comment it if necessary). Note that you have to answer the questions in the pdf report, including the numbers you get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Compute and report the entropy of each variable, and compare each value with its\n",
    "corresponding variable cardinality. What do you notice? Justify theoretically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "nck547oL14bW",
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.529809Z",
     "start_time": "2024-03-20T20:15:34.376033Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell only retrieve the data for later and defines a very useful function\n",
    "that computes a probability of n variables P[X1, ..., Xn].\n",
    "\"\"\"\n",
    "# retrieve the data\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# define useful function\n",
    "def get_probability_of(dataframe:pd.DataFrame, *variables:str):\n",
    "    shape = []\n",
    "    label_to_index = {}\n",
    "    for variable in variables:\n",
    "        uniques = dataframe[variable].unique()\n",
    "        for i, unique in enumerate(uniques):\n",
    "            label_to_index[unique] = i\n",
    "        shape.append(len(uniques))\n",
    "    probability_matrix = np.zeros(shape=shape)\n",
    "    labeled_joint_proba = (dataframe[list(variables)].value_counts() / len(df)).to_dict()\n",
    "    for key, value in labeled_joint_proba.items():\n",
    "        indexes = [label_to_index[k] for k in key]\n",
    "        probability_matrix.itemset(tuple(indexes), value)\n",
    "    return probability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.561345Z",
     "start_time": "2024-03-20T20:15:34.389846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of campaign_outcome = 0.999 (card. 2)\n",
      "Entropy of target_demographic = 1.585 (card. 3)\n",
      "Entropy of budget_allocation = 1.495 (card. 3)\n",
      "Entropy of reach = 1.563 (card. 3)\n",
      "Entropy of conversion_rate = 1.543 (card. 3)\n",
      "Entropy of campaign_duration = 1.546 (card. 3)\n",
      "Entropy of time_of_year = 2.000 (card. 4)\n",
      "Entropy of content_type = 1.548 (card. 3)\n",
      "Entropy of platform_used = 2.439 (card. 6)\n",
      "Entropy of weather = 1.455 (card. 3)\n",
      "Entropy of previous_campaign_outcome = 1.000 (card. 2)\n",
      "\n",
      "Knowing that:\n",
      "log_2(2) = 1.0\n",
      "log_2(3) = 1.584962500721156\n",
      "log_2(4) = 2.0\n",
      "log_2(6) = 2.584962500721156\n"
     ]
    }
   ],
   "source": [
    "for column_name in df.columns:\n",
    "    proba = get_probability_of(df, column_name)\n",
    "    cardinality = len(df[column_name].unique())\n",
    "    print(f\"Entropy of {column_name} = {entropy(proba):.3f} (card. {cardinality})\")\n",
    "\n",
    "print(\"\\nKnowing that:\")\n",
    "for i in [2, 3, 4, 6]:\n",
    "    print(f\"log_2({i}) = {np.log2(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Compute and report the conditional entropy of outcome given each of the other\n",
    "variables. Considering the variable descriptions, what do you notice when the\n",
    "conditioning variable is (a) weather and (b) previous_outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.563340Z",
     "start_time": "2024-03-20T20:15:34.414183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint entropy of campaign_outcome and target_demographic = 2.584\n",
      "Joint entropy of campaign_outcome and budget_allocation = 2.273\n",
      "Joint entropy of campaign_outcome and reach = 2.404\n",
      "Joint entropy of campaign_outcome and conversion_rate = 2.260\n",
      "Joint entropy of campaign_outcome and campaign_duration = 2.369\n",
      "Joint entropy of campaign_outcome and time_of_year = 2.999\n",
      "Joint entropy of campaign_outcome and content_type = 2.547\n",
      "Joint entropy of campaign_outcome and platform_used = 3.438\n",
      "Joint entropy of campaign_outcome and weather = 2.454\n",
      "Joint entropy of campaign_outcome and previous_campaign_outcome = 1.858\n"
     ]
    }
   ],
   "source": [
    "for column_name in df.columns[1:]:\n",
    "    proba = get_probability_of(df, \"campaign_outcome\", column_name)\n",
    "    print(f\"Joint entropy of campaign_outcome and {column_name} = {joint_entropy(proba):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Compute the mutual information between the variables target_demographic and\n",
    "budget. What can you deduce about the relationship between these two variables?\n",
    "What about the variables duration and reach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.564445Z",
     "start_time": "2024-03-20T20:15:34.440846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information of target_demographic and budget_allocation = 0.00031025276388252365\n",
      "Mutual information of campaign_duration and reach = 0.6039885926525139\n"
     ]
    }
   ],
   "source": [
    "proba = get_probability_of(df, \"target_demographic\", \"budget_allocation\")\n",
    "print(f\"Mutual information of target_demographic and budget_allocation = {mutual_information(proba)}\")\n",
    "proba = get_probability_of(df, \"campaign_duration\", \"reach\")\n",
    "print(f\"Mutual information of campaign_duration and reach = {mutual_information(proba)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "A student in Computer Science from the University of Li√®ge bets his friends that he\n",
    "can predict the upcoming election by accessing the dataset. However, his hacking\n",
    "skills are still weak. Therefore, he can only access a single variable of the dataset to\n",
    "make its prediction. Using only the mutual information, which variable should he\n",
    "choose to get? Would using conditional entropy lead to another choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.564622Z",
     "start_time": "2024-03-20T20:15:34.449026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information of target_demographic = 0.00046794421336437964\n",
      "Conditional entropy of target_demographic = 0.9985969873642466\n",
      "\n",
      "Mutual information of budget_allocation = 0.22078866087442206\n",
      "Conditional entropy of budget_allocation = 0.7782762707031883\n",
      "\n",
      "Mutual information of reach = 0.15847633111073822\n",
      "Conditional entropy of reach = 0.8405886004668729\n",
      "\n",
      "Mutual information of conversion_rate = 0.28282926976373135\n",
      "Conditional entropy of conversion_rate = 0.71623566181388\n",
      "\n",
      "Mutual information of campaign_duration = 0.17570790739621378\n",
      "Conditional entropy of campaign_duration = 0.8233570241813968\n",
      "\n",
      "Mutual information of time_of_year = 0.0004451007626644632\n",
      "Conditional entropy of time_of_year = 0.9986198308149461\n",
      "\n",
      "Mutual information of content_type = 0.0003241099012418758\n",
      "Conditional entropy of content_type = 0.9987408216763687\n",
      "\n",
      "Mutual information of platform_used = 0.0003861009113430569\n",
      "Conditional entropy of platform_used = 0.9986788306662672\n",
      "\n",
      "Mutual information of weather = 0.00010135823784329645\n",
      "Conditional entropy of weather = 0.998963573339767\n",
      "\n",
      "Mutual information of previous_campaign_outcome = 0.14101109113817678\n",
      "Conditional entropy of previous_campaign_outcome = 0.8580538404394341\n"
     ]
    }
   ],
   "source": [
    "for column_name in df.columns[1:]:\n",
    "    proba = get_probability_of(df, \"campaign_outcome\", column_name)\n",
    "    print(f\"Mutual information of {column_name} = {mutual_information(proba)}\")\n",
    "    print(f\"Conditional entropy of {column_name} = {conditional_entropy(proba)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater the mutual information the better (the other variable brings more information). The less the conditional entropy the better (the combination is less chaotic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "With the outcome of the previous campaign considered as known, would you change\n",
    "your answer from the previous question? What can you say about the amount of\n",
    "information provided by this variable? Compare this value with previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.635901Z",
     "start_time": "2024-03-20T20:15:34.477854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I(campaign_outcome ; target_demographic | previous_campaign_outcome) = 0.000\n",
      "I(campaign_outcome ; budget_allocation | previous_campaign_outcome) = 0.080\n",
      "I(campaign_outcome ; reach | previous_campaign_outcome) = 0.073\n",
      "I(campaign_outcome ; conversion_rate | previous_campaign_outcome) = 0.146\n",
      "I(campaign_outcome ; campaign_duration | previous_campaign_outcome) = 0.067\n",
      "I(campaign_outcome ; time_of_year | previous_campaign_outcome) = 0.001\n",
      "I(campaign_outcome ; content_type | previous_campaign_outcome) = 0.000\n",
      "I(campaign_outcome ; platform_used | previous_campaign_outcome) = 0.001\n",
      "I(campaign_outcome ; weather | previous_campaign_outcome) = 0.001\n",
      "\n",
      "H(campaign_outcome, target_demographic | previous_campaign_outcome) = 0.858\n",
      "H(campaign_outcome, budget_allocation | previous_campaign_outcome) = 0.778\n",
      "H(campaign_outcome, reach | previous_campaign_outcome) = 0.786\n",
      "H(campaign_outcome, conversion_rate | previous_campaign_outcome) = 0.712\n",
      "H(campaign_outcome, campaign_duration | previous_campaign_outcome) = 0.791\n",
      "H(campaign_outcome, time_of_year | previous_campaign_outcome) = 0.857\n",
      "H(campaign_outcome, content_type | previous_campaign_outcome) = 0.858\n",
      "H(campaign_outcome, platform_used | previous_campaign_outcome) = 0.857\n",
      "H(campaign_outcome, weather | previous_campaign_outcome) = 0.858\n"
     ]
    }
   ],
   "source": [
    "def joint_cond_entropy(Pxyz):\n",
    "    \"\"\"\n",
    "    Computes the conditional entropy of X knowing the joint probability of Y and Z \n",
    "    from the joint probability distribution Pxyz\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Pxyz: joint probability distribution of X, Y and Z\n",
    "            in a 3-D array where Pxyz[i][j][k]=P(X=i,Y=j,Z=k)\n",
    "    Return:\n",
    "    -------\n",
    "    - The joint conditional entropy H(X|Y,Z) as a number (integer, float or double)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute the sum along the X axis to get P[Y, Z]\n",
    "    Pyz = np.sum(Pxyz, axis=0)\n",
    "    for i in range(len(Pyz)):\n",
    "        Pyz[Pyz == 0] = 1 # replace the zeroes by non-zeroes, it will have no effect\n",
    "\n",
    "    # Compute the conditional probability\n",
    "    Px_given_yz = Pxyz / Pyz\n",
    "    for i in range(len(Px_given_yz)):\n",
    "        Px_given_yz[Px_given_yz == 0] = 1 # replace the zeroes by non-zeroes, it will have no effect\n",
    "    return -np.sum(Pxyz * np.log2(Px_given_yz)) # H(X|Y,Z) = -sum( P[X,Y,Z] * log_2(P[X|Y,Z]) )\n",
    "\n",
    "for column_name in df.columns[1:-1]:\n",
    "    proba = get_probability_of(df, \"campaign_outcome\", column_name, \"previous_campaign_outcome\")\n",
    "    print(f\"I(campaign_outcome ; {column_name} | previous_campaign_outcome) = {cond_mutual_information(proba):.3f}\")\n",
    "print()\n",
    "for column_name in df.columns[1:-1]:\n",
    "    proba = get_probability_of(df, \"campaign_outcome\", column_name, \"previous_campaign_outcome\")\n",
    "    print(f\"H(campaign_outcome, {column_name} | previous_campaign_outcome) = {joint_cond_entropy(proba):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Playing with information theory-based strategy\n",
    "Let us first consider a fictional game in which players have to guess all numbers (N, from 1\n",
    "to N) of a C x C grid. The same number may appear several times. At each turn, the player\n",
    "chooses to guess a single number in one of the fields (or cells). The game then lets the\n",
    "player know if the number is in the correct spot (green), if it is not in the grid (red), or if it is in\n",
    "the grid but at the wrong spot (orange)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 11\n",
    "In this simple version of the game, what is the entropy of each of the fields for a grid with C=3 and N=9? Also, what is the entropy of the whole game (the 9 fields combined) ? How are these two quantities linked? Justify."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.169925001442312\n"
     ]
    }
   ],
   "source": [
    "from math import comb\n",
    "xiProbs = [1/9] * 9\n",
    "entropy_value = entropy(np.array(xiProbs))\n",
    "print (entropy_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.636189Z",
     "start_time": "2024-03-20T20:15:34.532333Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.52932501298081\n",
      "28.529325012980813\n"
     ]
    }
   ],
   "source": [
    "totalCombinations = 9**9\n",
    "entropy_original = np.log2(totalCombinations)\n",
    "sum_entropies = entropy_value*9\n",
    "print(sum_entropies)\n",
    "print(entropy_original)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.636312Z",
     "start_time": "2024-03-20T20:15:34.534971Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QUESTION 12. \n",
    "In this simple version of the game with C=3 and N=9, let us assume that your first guess is a 9 in the center. What is now the entropy of each field, and the entropy of the game at this stage for each of the three possible outcomes (red, green, orange)? How much information has this guess brought you in each case (in bits)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "27.0\n",
      "1.529325012980813\n"
     ]
    }
   ],
   "source": [
    "# Case red\n",
    "totalCombinations = 8**9\n",
    "entropy_red = np.log2(totalCombinations)\n",
    "infogain = entropy_original - entropy_red\n",
    "print(entropy_red/9)\n",
    "print(entropy_red)\n",
    "print(infogain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.636412Z",
     "start_time": "2024-03-20T20:15:34.539064Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.169925001442312\n",
      "25.359400011538497\n",
      "3.1699250014423157\n"
     ]
    }
   ],
   "source": [
    "# Case green\n",
    "totalCombinations = 9**8\n",
    "entropy_green = np.log2(totalCombinations)\n",
    "infogain = entropy_original - entropy_green\n",
    "print(entropy_green/8)\n",
    "print(entropy_green)\n",
    "print(infogain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.646089Z",
     "start_time": "2024-03-20T20:15:34.542624Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.529325012980813\n",
      "25.695029143454242\n",
      "2.8342958695265708\n"
     ]
    }
   ],
   "source": [
    "# Case orange\n",
    "x5Probs = [1/8]*8\n",
    "entropy_x5 = entropy(np.array(x5Probs))\n",
    "i = 1\n",
    "posXi = 0\n",
    "combinationsRestCells = 0\n",
    "for i in range (1,9): # size of subset\n",
    "    combinationsRestCells += comb(7,i)*(8**2)*(9**7) # all combinations of\n",
    "    posXi += comb(7,i-1)*8*(9**7)\n",
    " \n",
    "probYi = posXi / (posXi + combinationsRestCells) \n",
    "probNYi = 1 - probYi\n",
    "\n",
    "xiProbs = [1/8]*8 # case Xi!=9\n",
    "# H(Xi|Y) = p(Y=0) * H (Xi| Y = 0) + p(Y=1) * H (Xi| Y = 1)\n",
    "# Second term is zero since knowing Y gives Xi\n",
    "entropy_cond_Xi_Y = probNYi * entropy (xiProbs) \n",
    "entropy_Y = -(probYi* np.log2(probYi) + probNYi*np.log2(probNYi))\n",
    "entropy_Xi = entropy_Y + entropy_cond_Xi_Y\n",
    "entropy_orange = entropy_x5 + 8 * entropy_Xi\n",
    "infogain = entropy_original - entropy_orange\n",
    "\n",
    "print(entropy_original)\n",
    "print (entropy_orange)\n",
    "print(infogain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.646328Z",
     "start_time": "2024-03-20T20:15:34.547945Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QUESTION 13\n",
    "In this simple version of the game with C=3 and N=9, let us now assume that your second guess is a 2 in the top-left corner. What is now the entropy of each field, and the entropy of the game at this stage for the following outcome? How much information has this second guess brought you (in bits)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.625\n"
     ]
    }
   ],
   "source": [
    "entropy_X5 = 0 # we know it's a 9\n",
    "prob_X1 = [1/8]*8 # all same prob except 2\n",
    "entropy_X1= entropy(prob_X1)\n",
    "print(entropy_X1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.646443Z",
     "start_time": "2024-03-20T20:15:34.551530Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.359400011538497\n",
      "22.48512495958259\n",
      "2.874275051955909\n"
     ]
    }
   ],
   "source": [
    "# same procedure as before\n",
    "combinationsRestCells = 0\n",
    "posXi = 0\n",
    "for i in range (1,8): # size of subset of possible combinations of 2s in 6 slots\n",
    "    combinationsRestCells += comb(6,i)*(8**2)*(9**6) # all combinations of 2s \n",
    "    posXi += comb(6,i-1)*8*(9**6)\n",
    "    \n",
    "probYi = posXi / (posXi + combinationsRestCells) # case: Xi = 2\n",
    "probNYi = 1 - probYi\n",
    "xiProbs = [1/8]*8 # case: Xi != 2\n",
    "# H(Xi|Y) = p(Y=0) * H (Xi| Y = 0) + p(Y=1) * H (Xi| Y = 1)\n",
    "entropy_cond_Xi_Y = probNYi * entropy (xiProbs)  # Knowing Y gives Xi\n",
    "entropy_Y = -(probYi * np.log2(probYi) + probNYi * np.log2(probNYi))\n",
    "entropy_Xi = entropy_Y + entropy_cond_Xi_Y\n",
    "entropy_GO = entropy_X5 + entropy_X1 + 7 * entropy_Xi\n",
    "infogain = entropy_green - entropy_GO\n",
    "\n",
    "print (entropy_green)\n",
    "print (entropy_GO)\n",
    "print(infogain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.646544Z",
     "start_time": "2024-03-20T20:15:34.556459Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now consider an advanced game in which the numbers in the grid to guess follows the same constraints as the Sudoku game (i.e., the same number can not appear twice on the same row, the same column, and the same sub 3x3 grid).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QUESTION 14\n",
    "In this advanced version of the game with C=3 and N=9, let us assume that your first guess is a 9 in the center and the second guess is a 2 in the top-left corner. What is now the entropy of each field, and the entropy of the game at every stage (including the initial stage) for the following outcome? How much information have these guesses brought you in this case (in bits)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0521258910921767\n",
      "18.46913301982959\n"
     ]
    }
   ],
   "source": [
    "# Initial case\n",
    "totalcases = 1\n",
    "for i in range(1,10):\n",
    "    totalcases *= i \n",
    "entropy_original_dep = np.log2(totalcases)\n",
    "entropy_Xi = entropy_original_dep/9\n",
    "print (entropy_Xi)# only for visualization\n",
    "print (entropy_original_dep)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.650315Z",
     "start_time": "2024-03-20T20:15:34.562524Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.29920801838728\n",
      "18.46913301982959\n",
      "3.169925001442312\n"
     ]
    }
   ],
   "source": [
    "# First guess\n",
    "entropy_X5 = 0 # we know it is a 9\n",
    "entropy_green_dep = entropy_X5\n",
    "totalcases = 1\n",
    "for i in range(1,9):\n",
    "    totalcases *= i\n",
    "    \n",
    "entropy_green_dep += np.log2(totalcases)\n",
    "fieldEntropy = entropy_green_dep/8 \n",
    "infogain = entropy_original_dep - entropy_green_dep\n",
    "\n",
    "print(entropy_green_dep)\n",
    "print (entropy_original_dep)\n",
    "print(infogain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.653888Z",
     "start_time": "2024-03-20T20:15:34.566412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.748120982967706\n",
      "15.29920801838728\n",
      "2.551087035419574\n"
     ]
    }
   ],
   "source": [
    "# Second guess\n",
    "x1Probs = [1/7]*7 \n",
    "entropy_X1 = entropy(x1Probs)# we known it is neither a 2 nor a 9 \n",
    "unknownCells = 7\n",
    "entropy_GO_dep = entropy_X1\n",
    "for i in range(1,7):\n",
    "    rest = unknownCells -i \n",
    "    posYi = (rest**rest)\n",
    "    posNYi = rest*((rest+1)**(rest-1))\n",
    "    probYi = posYi/(posYi+posNYi)\n",
    "    probNYi = 1- probYi\n",
    "    \n",
    "    xiProbs = [1/rest]*rest\n",
    "    entropy_cond_Xi_Y = probNYi * entropy(xiProbs) \n",
    "    entropy_Y = -(probYi* np.log2(probYi) + probNYi*np.log2(probNYi))\n",
    "    entropy_Xi = entropy_Y + entropy_cond_Xi_Y\n",
    "    entropy_GO_dep += entropy_Xi\n",
    "\n",
    "infogain = entropy_green_dep - entropy_GO_dep\n",
    "\n",
    "print(entropy_GO_dep)\n",
    "print(entropy_green_dep)\n",
    "print(infogain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.654001Z",
     "start_time": "2024-03-20T20:15:34.571363Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QUESTION 15\n",
    "In this advanced version of the game with C=9 and N=9, what is the entropy of an\n",
    "empty grid? How does it compare to the grid in the simple version?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.49837221884734\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "totalCombinations = 6670903752021072936960 # looked up on wiki\n",
    "entropy = log2(6670903752021072936960) # number is too big that numpy cannot process it\n",
    "print(entropy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:15:34.654101Z",
     "start_time": "2024-03-20T20:15:34.575328Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
